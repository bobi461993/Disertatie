\chapter{Arhitectura}

\paragraph{}
Modelarea unui limbaj reprezint\u a o sarcin\u a extrem de dificil\u a pentru un calculator. Recente progrese \^ in aria Deep Learning au f\u acut posibile diverse dezvolt\u ari \^ in aceast\u a direc\c tie, \^ ins\u a lucrurile sunt departe de a fi rezolvate. Arhitectura folosit\u a pentru construc\c tia chatbot-ului prezentat \^ in aceast\u a lucrare va fi expus\u a precum un bloc de construc\c tii, plec\^ and de la no\c tiunile de baz\u a, p\^ an\u a la arhitectura final\u a.

\section{Re\c tele neurale artificiale}

\paragraph{}
O re\c tea neural\u a artificial\u a (RNA) reprezint\u a o paradigm\u a bazat\u a pe procesare de informa\c tii a c\u arei inspira\c tii provine din sistemul nervos biologic. Precum creierul uman, o RNA este compus\u a dintr-un num\u ar mare de elemente de procesare interconectate (neuroni) lucr\^ and la unison pentru a rezolva diverse probleme. RNA, precum oamenii, \^ inva\c t\u a din exemple. \^ Inv\u a\c tarea \^ in sistemele biologice implic\u a ajustarea conexiunilor sinaptice care exist\u a \^ intre neuroni. Acest principiu se aplic\u a \c si acestor re\c tele.

\paragraph{}
Ca \c si modelare matematic\u a propriu-zis\u a, o RNA poate fi observat\u a \^ in Figura 3.1. Avem o intrare reprezentat\u a \^ in figur\u a de vectorul n-dimensional (x1, x2, x3, x4). Aceast\u a intrare reprezint\u a caracteristicile unui e\c santion care face parte dintr-un set de date. Straturile intermediare se numesc straturi ascunse, iar rolul lor este s\u a produc\u a o abstractizare c\^ at mai complex\u a a datelor de intrare, folosind diverse func\c tii cu activare neliniar\u a. Ultimul strat se nume\c ste strat de ie\c sire, reprezent\^ and eticheta (clasa) vectorului de intrare.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{deep_neural_network}
\caption{Exemplu de re\c tea neural\u a artificial\u a}
\end{figure}

\paragraph{}
Ceea ce de fapt acest tip de re\c tele incearc\u a s\u a inve\c te sunt ponderile dintre straturile sale. Se pleac\u a de la un set de ponderi alese aleator \footnote{Valorile ponderilor sunt deobicei subunitare iar ini\c tializarea este f\u acut\u a urm\^ and diverse principii matematice bine definite. Pentru cazuri simpliste, ini\c tializarea poate fi totu\c si facut\u a folosind o distribu\c tie uniform\u a.} \c si se ajusteaz\u a folosind algoritmul de propagare \^ inapoi a erorii p\^ an\u a c\^and re\c teaua se stabilizeaz\u a. 

\section{Re\c tele neurale artificiale recurente}

\paragraph{}
Dup\u a cum se poate observa, o RNA este util\u a atunci c\^ and intrarea este format\u a dintr-un singur element, de exemplu o imagine. Modelarea unui limbaj \^ ins\u a presupune ca intr\u arile s\u a fie fraze. O fraz\u a este format\u a din mai multe elemente (cuvinte), deci o RNA nu poate modela o astfel de intrare. Solu\c tia pentru aceast\u a problem\u a este oferit\u a de re\c telele neurale artificiale recurente (RNAR). Acestea sunt folosite pentru a modela secven\c te unde exist\u a o dependen\c t\u a temporal\u a (fraze, serii de timp). Fiecare intrare curent\u a din secven\c t\u a este condi\c tionat\u a de cele precedente. O astfel de re\c tea se poate observa \^ in Figura 3.2. Straturile ascunse \^ intr-o RNAR au rolul de a p\u astra o captur\u a a tot ceea ce s-a oferit ca intrare p\^ an\u a \^ in momentul curent. Valoarea fiec\u arui strat ascuns curent depinde astfel at\^ at de intrarea curent\u a c\^ at \c si de ie\c sirea stratului ascuns anterior. Putem considera toat\u a aceast\u a modelare ca pe o probabilitate condi\c tionat\u a \( P(x_{n} | x_{1}, x_{2},..., x_{n-1}) \), unde \^ in cazul unei fraze \(x_{1}, x_{2},..., x_{n}\) reprezint\u a cuvintele acesteia. 

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{rnn_network}
\caption{Exemplu de re\c tea neural\u a artificial\u a recurent\u a}
\end{figure}

\paragraph{}
Algoritmul de reglare a ponderilor se nume\c ste propagarea \^ inapoi \^ in timp a erorii. O problem\u a serioas\u a care a ap\u arut odat\u a cu introducerea acestor re\c tele se nume\c ste risipirea gradien\c tilor. \^ In cazul \^ in care secven\c ta de intrare are o lungime mare, informa\c tia nu reu\c se\c ste s\u a se propage \^ in timp deoarece gradientul ponderilor se risipe\c ste, ponderile devenind 0. Mecanismul care \^ inl\u atur\u a aceast\u a problem\u a se nume\c ste Long Short Term Memory (LSTM). Principiul este ca prin folosirea unor por\c ti de transmitere a informa\c tiei, gradientul s\u a fie stabilizat. Acest mecanism a reprezentat un avans spectaculos pentru Deep Learning, permi\c t\^ and modelarea secven\c tial\u a cu re\c tinere a informa\c tiei pe o perioad\u a \^ indelungat\u a de timp. Ecua\c tiile 3.1 descriu calculele pentru por\c tile LSTM.

\begin{equation}
\begin{split}
f_{t} = \sigma(W_{f}x_{t} + U_{f}h_{t-1} + b_{f})\\
i_{t} = \sigma(W_{i}x_{t} + U_{i}h_{t-1} + b_{i})\\
o_{t} = \sigma(W_{o}x_{t} + U_{o}h_{t-1} + b_{o})\\
c_{t} = f_{t} \circ c_{t-1} + i_{t} \circ \tanh (W_{c}x_{t} + U_{c}h_{t-1} + b_{c})\\
h_{t} = o_{t} \circ \tanh (c_{t})\\
\end{split}
\end{equation}

\paragraph{}
Poarta \(f_{t}\) se nume\c ste poart\u a de uitare (forget gate). Rolul ei este s\u a stabilieasc\u a c\^ at\u a informa\c tie se va uita din trecut. Poarta \(i_{t}\) este poarta de intrare (input gate). Aceasta delimiteaz\u a care este cantitatea de informa\c tie care se p\u astreaz\u a din intrarea curent\u a. Poarta \(o_{t}\) este cea de ie\c sire (output gate). Ea controleaz\u a ce informa\c tie va fi transmis\u a c\u atre ie\c sire. \(c_{t}\) se nume\c ste starea intern\u a a celulei LSTM. Aceasta este calculat\u a ca o combina\c tie \^ intre starea anterioar\u a a celulei, poarta de intrare \c si cea de ie\c sire. \(h_{t}\) reprezint\u a ie\c sirea curent\u a a re\c telei \c si depinde de poarta de ie\c sire \c si starea celulei LSTM. 

\paragraph{}
O RNAN aduce mai aproape ideea de modelare lingvistica, \^ ins\u a se poate observa c\u a aceasta nu permite ca intrare dec\^ at o fraz\u a pe r\^ and. Modelarea dorit\u a este o pereche de forma \^ intrebare-r\u aspuns.

\section{Modele Sequence-to-Sequence}

\paragraph{}
Transla\c tia a reprezentat mereu un punct de interes \^ in mediul proces\u arii naturale de limbaj, constituind de altfel o provocare uria\c s\u a de-a lungul ultimilor ani. P\^ an\u a \^ in anul 2014, majoritatea modelelor de transla\c tie se bazau pe lan\c turi Markov ascunse (Hidden Markov Models - HMM), totul urm\^ and a se schimba odat\u a cu introducerea unei noi arhitecturi \^ in acel an de c\u atre Cho et Al. Noul tip de arhitectur\u a se numea Sequence-to-Sequence (seq2seq) \c si urma s\u a aduc\u a imbun\u at\u a\c tiri spectaculoase at\^ at pe partea de transla\c tie c\^ at \c si pe partea conversa\c tional\u a. Modelul este vizibil \^ in Figura 3.3.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{seq2seq}
\caption{Modelul Sequence-to-Sequence}
\end{figure}

\paragraph{}
Aceast tip de arhitectur\u a este \^ impar\c tit\u a \^ in dou\u a jum\u at\u a\c ti: codor (encoder) \c si decodor (decoder). Sarcina codorului este s\u a primeasc\u a o fraz\u a cu un num\u ar variabil de cuvinte ca intrare iar unica sa ie\c sire \footnote{Ie\c sirea ultimului element din secven\c t\u a} s\u a fie o captur\u a a \^ intregii intrari (un vector de lungime fix\u a). Aceast\u a captur\u a poate fi privit\u a ca o sumarizare a \^ intregii fraze. Sarcina decodorului este de a \^ inv\u a\c ta fraze pornind de la ie\c sirea codorului, care va deveni prima intrare din decodor, \c si restul intrarilor precedente. Intrarea curent\u a \(w_{n}\) \^ in decodor este ie\c sirea de la timpul anterior, \(w_{n-1}\). Modelarea se transform\u a astfel \^ intr-o probabilitate condi\c tionat\u a: \(P(w_{n} | w_{1}, w_{2},..., w_{n-1}, c)\), unde \(c\) reprezint\u a ie\c sirea codorului, deseori \^ int\^ alnit sub numele de context \^ in literatura de specialitate.

\paragraph{}
Intr\u arile \^ in acest model sunt de forma \^ intrebare-r\u aspuns. Original folosit ca model de transla\c tie, unde intrarea pentru codor era fraza \^ intr-o limb\u a iar intrarea \^ in decodor era fraza tradus\u a \^ in limba dorit\u a, acest principiu se poate aplica la fel de u\c sor pentru a modela o conversa\c tie. Spre deosebire de traducere unde totul se \^ int\^ ampla punctual iar r\u aspunsul nu depinde dec\^ at de intrarea curent\u a, o conversa\c tie este foarte dependent\u a de un context. F\u ar\u a acest context, partenerul angrenat \^ in discu\c tie ar r\u aspunde mereu lu\^ and \^ in considerare doar ce i s-a spus \^ in momentul de fa\c t\u a, ignor\^ and orice replic\u a anterioar\u a. Acesta nu este un comportament dorit \^ in cadrul unei conversa\c tii \c si de aceea modelul seq2seq nu este destul de puternic de sine st\u at\u ator pentru a modela o discu\c tie.

\section{Hierarchical Recurrent Encoder-Decoder}

\paragraph{}
Modelarea contextului discu\c tiei reprezint\u a una dintre principalele nevoi \^ in ceea ce prive\c ste o conversa\c tie care dore\c ste s\u a par\u a c\^ at mai real\u a. P\^ an\u a recent, cea mai apropiat\u a arhitectur\u a care realiza acest lucru era modelul seq2seq, \^ ins\u a contextul era re\c tinut doar la nivelul unei singure fraze. \^ In anul 2016, Iulian \c Serban et Al. a introdus re\c teaua Hierarchical Recurrent Encoder-Decoder (HRED - Figura 3.4). Ea poate fi vazut\u a ca o extensie peste seq2seq. Avantajul acesteia este c\u a poate re\c tine contextul discu\c tiei.

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{hred}
\caption{Hierarchical Recurrent Encoder-Decoder}
\end{figure} 

\paragraph{}
Dup\u a cum se poate observa, modelul este construit prin al\u aturarea mai multor re\c tele seq2seq legate \^ intre ele printr-o RNAR contextual\u a. Dup\u a cum sugereaz\u a \c si numele, rolul acestei RNAR este de a re\c tine contextul discu\c tiei de-a lungul mai multor fraze. Spre deosebire de seq2seq, ie\c sirea codorului nu mai este oferit\u a direct ca \c si prim\u a intrare pentru decodor, aceasta trec\^ and mai \^ intai prin RNAR contextual\u a. Pentru a fi mai u\c sor de \^ inteles ce se \^ intampl\u a, ne putem imagina toate cele \(n\) codoare \c si decodoare ca fiind simple unit\u a\c ti de intrare, respectiv ie\c sire \^ intr-o RNAR obi\c snuit\u a. Precum sunt \^ intr-o RNAR ie\c sirile dependente at\^ at de intrarea curent\u a, c\^ at \c si de cele precedente, prin analogie putem deduce c\u a \^ intr-o re\c tea HRED, fiecare ie\c sire depinde de fraza curent\u a \c si de contextul discu\c tiei (frazele precedente).

\subsection{Celula GRU}

\paragraph{}
TODO

\subsection{HRED bidirec\c tional}

\paragraph{}
Rolul codorului, precum a fost men\c tionat mai devreme, este s\u a captureze informa\c tia unei fraze \^ intr-un vector de lungime fix\u a. \^ In orice limbaj, sensul unui cuv\^ ant nu este stabilit doar de cuvintele precedente ci \c si de cele ce vor urma. Deoarece RNAR modeleaz\u a cuvintele dintr-o fraz\u a pornind de la cuv\^ antul \(w_{1}\) la \(w_{n}\), \^ in\c telesul din viitor al acestora este ignorat, iar captura frazei poate s\u a nu fie \^ indeajuns de reprezentativ\u a. Astfel, este propus\u a folosirea unei RNAR bidirec\c tionale (Figura 3.5) pentru codor. Acest tip de re\c tea folose\c ste dou\u a parcurgeri ale secven\c tei de intrare: cea \^ inainte, care se desf\u a\c soar\u a \^ in mod obi\c snuit \c si cea invers\u a (de la \(w_{n}\) la \(w_{1}\)), pentru capturarea \^ intelesului din viitor al cuvintelor. Ie\c sirea pentru pargurgerea \^ inainte va fi la timpul \(n\) iar cea pentru parcurgerea invers\u a va fi la timpul ini\c tial. Vectorul de lungime fix\u a va fi format din concatenarea celor dou\u a ie\c siri ale re\c telei bidirec\c tionale.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{bidirectional_rnn}
\caption{RNAR bidirec\c tional}
\end{figure} 

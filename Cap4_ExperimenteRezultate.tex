\chapter{Experimente \c si rezultate}

\paragraph{}
Acest capitol prezint\u a configura\c tiile arhitecturale \^ incercate, cele care au avut succes precum \c si rezultatele finale ob\c tinute de c\u atre sistem. Pentru validarea modelului ob\c tinut s-a realizat o scalare progresiv\u a a num\u arului de date de intrare, acesta fiind supus la dou\u a teste principale. Unul dintre teste a fost ca pentru pu\c tine exemple de intrare, modelul s\u a fie capabil s\u a \^ inve\c te mot a mot s\u a produc\u a r\u aspunsuri precum \^ in setul de antrenare (overfitting). Un alt test a fost ca atunci c\^ and num\u arul de intr\u ari este crescut considerabil, sistemul s\u a nu produc\u a \^ intotdeauna r\u asupunsuri monotone. Metricile de evaluare principale ale sistemului au fost cea uman\u a \c si word perplexity \cite{Bengio:2003:NPL:944919.944966}.

\section{Set de date}

\paragraph{}
Setul de date utilizat pentru experimente se nume\c ste Movie Triples \cite{DBLP:journals/corr/SerbanSBCP15}, ob\c tinut prin expandarea \c si preprocesarea setului de date ini\c tial numit Movie-DiC \cite{Banchs:2012:MMD:2390665.2390716} \c si, dup\u a cum indic\u a numele, reprezint\u a un set de date care folose\c ste subtitr\u ari din filme. Deoarece chatbot-ul descris \^ in aceast\u a lucrare \^ i\c si propune s\u a fie unul open-topic, tipul conversa\c tiilor din filme alc\u atuiesc setul de date ideal pentru acest obiectiv. Desigur, setul de date nu este din categoria celor foarte mari, con\c tin\^ and aproximativ 200.000 de intr\u ari, dar suficiente pentru a modela un limbaj de o anvergur\u a mai mic\u a. Vocabularul setului de date con\c tine 10.000 de cuvinte; cuvintele din afara acestui vocabular sunt marcate cu simbolul unk (unknown). Intr\u arile sunt de forma triplete de fraze \((F_1, F_2, F_3)\), captur\^ and astfel contextul discu\c tiei pe o \^ intindere de trei replici. \^ In cazul \^ in care ar exista o pereche de \(n\) fraze pentru o intrare, HRED este capabil s\u a captureze contextul de-a lungul acestora.

\section{Mediu de lucru/Framework}

\paragraph{}
Pentru dezvoltarea modelului s-a folosit framework-ul Keras \cite{chollet2015keras} cu backend de Tensorflow \cite{DBLP:journals/corr/AbadiABBCCCDDDG16} \^ in limbajul de programare Python. Keras este un framework pentru Deep Learning, preferat de majoritatea comunit\u a\c tii \c stiin\c tifice de Deep Learning, care a fost g\^ andit \c si construit pentru u\c surarea muncii de cercetare \c si prototipizarea rapid\u a a modelelor. Astfel, efortul de concentrare este orientat spre descoperirea arhitecturii potrivite pentru modelarea problemei necesare \c si foarte pu\c tin pe latura de software development. De asemenea, framework-ul este dezvoltat astfel \^ inc\^ at s\u a profite de capacitatea de procesare GPU pe care sistemul hardware o pune la dispozi\c tie, procesul de instruire fiind accelerat de zeci de ori.

\section{Arhitecturi canditat}

\subsection{Arhitectura 1.0}

\paragraph{}
Arhitectura ini\c tial\u a presupunea folosirea unei dimensiuni maxime pentru o fraz\u a de 75 de cuvinte. Deoarece frazele au dimensiuni variabile de lungime, iar o RNAR accept\u a dimensiuni fixe, pentru a completa fraza p\^ an\u a la lungimea maxim\u a aleas\u a, se umpleau spa\c tiile lips\u a cu vectori \(0\), procedeu denumit \c si padding \^ in literatur\u a. Fiecare fraz\u a se sf\^ ar\c se\c ste cu un simbol special numit end token iar apoi urmeaz\u a padding-ul.  Fiecare strat de ie\c sire RNAR din HRED (codor, context \c si decodor) avea o dimensiune de 256 de neuroni. De remarcat este faptul c\u a fiecare unitate structural\u a din HRED, re\c telele recurente folosite pentru codor \c si decodor, foloseau ponderi shared\footnote{Fiecare codor din HRED folosea acela\c si set de ponderi, precum \c si \^ in cazul decodorului}. Stratul de embedding pentru cuvinte pornea cu \^ inv\u a\c tarea de la 0, nefolosindu-se word embeddings provenite de la word2vec. Ie\c sirea din RNAR contextual\u a era concatenat\u a cu reprezentarea fiec\u arui cuv\^ ant din decodor. Ie\c sirea era mai apoi procesat\u a de un strat ascuns cu activare \(\tanh\) pentru expandarea dimensiunii \c si rezultatul era concatenat cu restul secven\c tei de intrare pentru decodor. Algoritmul pentru \^ inv\u a\c tarea ponderilor folosit a fost RMSprop \cite{rmsprop}. Pentru predi\c tie s-a folosit\u a varianta greedy descris\u a \^ in Capitolul 3. Aceast\u a versiune performa bine atunci c\^ and erau date pu\c tine, \^ ins\u a odat\u a cu cre\c sterea volumului, modelul nu reu\c sea s\u a scaleze.

\subsection{Arhitectura 1.5}

\paragraph{}
Principala deosebire fa\c t\u a de versiunea precedent\u a a fost folosirea word embeddings pentru reprezentarea cuvintelor \c si inghe\c tarea stratului respectiv pentru a nu modifica ponderile preantrenate. S-a dovedit c\u a aceast\u a alegere a \^ imbun\u at\u a\c tit at\^ at timpul de antrenare, deoarece num\u arul parametrilor a fost redus considerabil, c\^ at \c si performan\c ta modelului, acesta fiind capabil de a captura un num\u ar mai mare de date, dar totu\c si insuficient. Algoritmul pentru optimizarea ponderilor folosit a fost \^ inlocuit cu Adam \cite{DBLP:journals/corr/KingmaB14} folosind constanta de \^ inv\u a\c tare implic\u a (\(0.001\)), care se presupune c\u a ob\c tine o convergen\c t\u a mai bun\u a fa\c t\u a de RMSprop. De asemenea s-a \^ incercat folosirea simbolului special de \^ inceput de fraz\u a denumit \c si start token, \^ ins\u a s-a observat c\u a ie\c sirea contextual\u a era \^ intotdeauna \^ inclinat\u a c\u atre acest simbol \^ in timpul predic\c tiei, sistemul nemaifiind capabil s\u a modeleze fraza dorit\u a ca r\u aspuns.

\subsection{Arhitectura 2.0}

\paragraph{}
Deoarece modelul e\c sua \^ in a scala atunci c\^ and num\u arul de date cre\c stea, presupunerea a fost c\u a acesta nu era \^ indeajuns de complex pentru a reprezenta distribu\c tia setului de antrenare. De aceea, s-a \^ incercat folosirea unor straturi suplimentare alipite vertical\footnote{Stacked} pentru codor \c si decodor deoarece teoria spune c\u a de la strat la strat, modelul captureaz\u a reprezent\u ari tot mai complexe ale intr\u arii \cite{cnn-features}. Procesul de instruire a devenit astfel foarte lent, deoarece complexitatea arhitecturii a crescut, \^ ins\u a rezultatul final era nesatisf\u ac\u ator. 

\subsection{Arhitectura 3.0}

\paragraph{}
Arhitectura 3.0 a adus cu sine un progres important spre atingerea obiectivului final. S-a renun\c tat astfel la straturi stacked deoarece nu rezolvau problema scalabilit\u a\c tii. Noua configura\c tie pentru fiecare ie\c sire RNAR era 300 de neuroni pentru codor, 300 de neuroni pentru context \c si 300 pentru decodor. S-a renun\c tat de asemenea la concatenarea ie\c sirii contextuale cu fiecare reprezentare a cuvintelor din decodor \c si trecerea acesteia prin stratul ascuns cu activare \(\tanh\), deoarece nu mai era nevoie de expandarea dimensiunii. Astfel, ie\c sirea contextual\u a era folosit\u a acum doar ca prim\u a intrare \(T_1\) pentru secven\c ta decodorului, presupunerea fiind c\u a influen\c ta contextual\u a se propag\u a oricum prin RNAR a decodorului. \^ Inc\u a un considerent pentru aceast\u a decizie a fost faptul c\u a ie\c sirea contextual\u a \c si reprezentarea word2vec a cuvintelor provin din distribu\c tii diferite iar \^ impreunarea lor nu reprezenta un factor corect din punct de vedere matematic. Desigur, mecanismele de aten\c tie descrise \^ in Capitolul 5 posed\u a o putere de modelare sporit\u a fa\c t\u a de simpla utilizare a contextului ca prim\u a intrare pentru decodor. \^ Inc\u a o observa\c tie realizat\u a care \^ incetinea procesul de antrenare era folosirea unei fraze cu o lungime maxim\u a de 75 de cuvinte. \^ In urma analizei amanun\c tite a setului de date, s-a observat c\u a majoritatea frazelor aveau o lungime scurt\u a. Astfel, s-a calculat valoarea de \(80\%\) (80 percentile) \cite{johnson2007elementary} pentru lungimea maxim\u a a frazelor \c si s-a determinat c\u a aceasta este aproximativ \(25\). Sacrificiul realizat pentru a ignora cuvintele frazelor care dep\u a\c sesc lungimea de 25 de cuvinte a produs o sc\u adere considerabil\u a a timpului de instruire. Odat\u a cu schimbarea c\u atre aceast\u a configura\c tie, s-a adoptat algoritmul beam search de predic\c tie. Deoarece spa\c tiul de c\u autare devenea mai larg, r\u aspunsurile au devenit \c si ele mai diversificate. Valorile pentru num\u arul de copii genera\c ti de fiecare nod \c si beam size au fost alese 2, respectiv 5. \^ In teorie, cu c\^ at aceste numere sunt mai mari, spa\c tiul explorat cre\c ste \c si r\u aspunsul devine mai calitativ. \^ Ins\u a cre\c sterea considerabil\u a a acestor doi parametri aduce cu sine o vitez\u a computa\c tional\u a lent\u a, de aceea trebuie f\u acut un compromis la nivelul acestor valori. \^ In pofida acestor \^ imbun\u at\u a\c tiri, sistemul \^ inc\u a nu scala \^ in mod a\c steptat odat\u a cu cre\c sterea datelor.

\subsection{Arhitectura 3.5}

\paragraph{}
Deoarece setul de date con\c tine subtitr\u ari din filme, este un lucru obi\c snuit ca majoritatea r\u aspunsurilor s\u a fie monotone. Astfel, se creeaz\u a o tendin\c t\u a pentru anumite cuvinte care apar foarte des, cele rare fiind aproape ignorate de c\u atre sistem. De aceea, sistemul e\c sueaz\u a \^ in a furniza r\u aspunsuri diversificate, care s\u a con\c tin\u a \c si cuvinte rare. Acest neajuns se poate rezolva prin folosirea ponderilor pentru cuvinte \^ in momentul c\^ and este calculat\u a valoarea func\c tiei de eroare. Ponderea pentru fiecare cuv\^ ant va fi num\u arul de apari\c tii \(/\) num\u arul total de cuvinte. Prin aceast\u a metod\u a se asigur\u a c\u a sistemul consider\u a influen\c ta egal\u a a tuturor cuvintelor nemaiav\^ and o predispozi\c tie c\u atre cele care apar des. Dimensiunea ie\c sirilor codor, context, decodor au fost alese ca fiind 500, 300, 500 pentru cre\c sterea u\c soar\u a a complexit\u a\c tii modelului. \^ In urma antren\u arii se putea observa c\u a modelul r\u am\^ ane blocat \^ in anumite regiuni de minim local. Acest aspect a fost \^ inl\u aturat prin reducerea progresiv\u a a constantei de \^ inv\u a\c tare de-a lungul instruirii, atunci c\^ and valoarea func\c tiei de eroare nu sc\u adea pe parcursul a cinci epoci consecutive cu mai mult de o valoare \(\epsilon\) aleas\u a. Factorul de reducere a fost ales ca fiind \(75\%\). Aceast\u a combina\c tie de ponderi \^ impreun\u a cu reducerea treptat\u a a constantei de \^ inv\u a\c tare au reprezentat succesul care au produs convergen\c ta modelului curent \c si totodat\u a apari\c tia rezultatelor satisf\u ac\u atoare.

\section{Rezultate}

\subsection{Word perplexity}

\paragraph{}
Evaluarea precis\u a a modelelor lingvistice care nu sunt bazate pe un topic restr\^ ans reprezint\u a o problem\u a deschis\u a. Nu exist\u a o metric\u a bine stabilit\u a pentru evaluarea automat\u a, iar evaluarea uman\u a este costisitoare. Cu toate acestea, pentru modele probabilistice ligvistice, word perplexity \cite{Bengio:2003:NPL:944919.944966} este o metric\u a recunoscut\u a, fiind sugerat\u a \c si cu alte ocazii pentru modele de dialog generative. Word perplexity este definit ca:

\begin{equation}
perplexity = e^{-\sum_{x}\tilde{p}(x)\log{q(x)}}
\end{equation}

\paragraph{}
Exponentul ecua\c tiei 4.1 reprezint\u a valoarea de cross entropy a modelului evaluat pe setul de date de test. O valoarea perplexity mai mic\u a este indica\c tia unui model mai bun. Metrica m\u asoar\u a abilitatea modelului de a lua \^ in considerare structura sintactic\u a a dialogului (schimbul de replici), precum \c si structura sintactic\u a a fiec\u arei fraze (semnele de punctua\c tie). \^ In generarea de dialog, distribu\c tia cuvintelor din fraza urm\u atoare este multi-modal\u a (exist\u a o mul\c time de r\u aspunsuri potrivite), aspect ce recomand\u a folosirea acestei metrici, deoarece aceasta va m\u asura \^ intotdeauna probabilitatea de regenerare a exact frazei de referin\c t\u a. Modelul curent a ob\c tinut o valoare word perplexity de \textbf{118.74} pentru setul de test. Acest num\u ar se poate traduce prin faptul c\u a modelul este confuz ca \c si cum ar avea de ales uniform \c si independent dintre 119 posibilit\u a\c ti pentru fiecare cuv\^ ant.

\paragraph{}
Desigur, word perplexity nu reprezint\u a metrica ideal\u a de evaluare a unui model ligvistic, deoarece un model cu o valoare mic\u a ob\c tinut\u a pe un set de test prezint\u a riscul de a nu performa la fel de bine \^ intr-o aplica\c tie real\u a a c\u arei date nu provin din aceea\c si distribu\c tie ca a setului de test. Totu\c si, \^ in lipsa unei metrici eficiente de a m\u asura performan\c ta unui sistem dialog, word perplexity ofer\u a o alternativ\u a suficient de bun\u a pentru moment.

\subsection{Evaluare uman\u a}

\paragraph{}
De\c si costisitoare din punct de vedere al timpului, evaluarea uman\u a reprezint\u a poate cea mai bun\u a metric\u a curent\u a de evaluare a unui model generativ lingvistic, \^ incadrandu-se \^ in acela\c si timp cu ideea din spatele testului Turing. \^ In urma gener\u arii mai multor conversa\c tii, s-au ales urm\u atoarele rezultate:

\begin{center}
  \begin{tabular}{ | m{6.5cm} | m{6.5cm} |}
    \hline
    Persoan\u a real\u a & Chatbot \\ \hline
    Hi! & Hi. \\ 
    Are you ok? & Yes \\ 
    Anything else? & Uh. Yeah. \\
    What is it? & Well, I'd have to go back to New York anyway. I spent so much money on television. \\
    \hline
  \end{tabular}
\end{center}

\begin{center}
  \begin{tabular}{ | m{6.5cm} | m{6.5cm} |}
    \hline
    Persoan\u a real\u a & Chatbot \\ \hline
    You are a shame. & I've never known a long way to do it anyway. \\ 
    Are you proud of yourself? & I want you to go back to the hotel. I want you to leave. For me. \\ 
    I will probably do that. & Well, I'm in town for a few years now. I'd like to go to sleep. \\
    \hline
  \end{tabular}
\end{center}

\begin{center}
  \begin{tabular}{ | m{6.5cm} | m{6.5cm} |}
    \hline
    Persoan\u a real\u a & Chatbot \\ \hline
    Do you know this guy? & He'll be here. I'm sorry for nothing to worry about. I'll survive some days. \\ 
    I hope I can trust you. & You don't understand. I don't believe you. \\ 
    I am a man of my word. & You know what I'm gonna do. \\
    \hline
  \end{tabular}
\end{center}

\begin{center}
  \begin{tabular}{ | m{6.5cm} | m{6.5cm} |}
    \hline
    Persoan\u a real\u a & Chatbot \\ \hline
    Are you a robot? & What are you doing? \\
    Do you have feelings? & I don't know what you are talking about. \\
    Tell me about yourself. & I know. It's not logical, it's not what I can do. Forget it. \\
    \hline
  \end{tabular}
\end{center}

\paragraph{}
Dup\u a cum se poate observa, r\u aspunsurile produse de c\u atre chatbot prezint\u a o corectitudine gramatical\u a \c si sintactic\u a suficient de bun\u a, \^ ins\u a acestea sufer\u a la capitolul coeren\c t\u a, put\^ andu-se observa c\u a anumite replici nu se \^ incadreaz\u a neaparat \^ in contextul discu\c tiei. Capitolul urm\u ator descrie c\^ ateva posibile \^ imbun\u at\u a\c tiri care ar putea suprima aceste probleme.



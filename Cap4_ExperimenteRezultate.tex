\chapter{Experimente \c si rezultate}

\paragraph{}
Acest capitol prezint\u a configura\c tiile arhitecturale \^ incercate, cele care au avut succes precum \c si rezultatele finale ob\c tinute de c\u atre sistem. Pentru validarea modelului ob\c tinut s-a realizat o scalare progresiv\u a a num\u arului de date de intrare, acesta fiind supus la dou\u a teste principale. Unul dintre teste a fost ca pentru pu\c tine exemple de intrare, modelul s\u a fie capabil s\u a \^ inve\c te mot a mot s\u a produc\u a r\u aspunsuri precum \^ in setul de antrenare (overfitting). Un alt test a fost ca atunci c\^ and num\u arul de intr\u ari este crescut considerabil, sistemul s\u a nu produc\u a \^ intotdeauna r\u asupunsuri monotone. Metrica de evaluare principal\u a a sistemului a fost cea uman\u a, ca \c si \^ in cazul testului Turing.

\section{Set de date}

\paragraph{}
Setul de date folosit pentru experimente se nume\c ste Movie Triples \c si, dup\u a cum indic\u a numele, reprezint\u a un set de date care folose\c ste subtitr\u ari din filme. Deoarece chatbot-ul descris \^ in aceast\u a lucrare \^ i\c si propune s\u a fie unul open-topic, tipul conversa\c tiilor din filme alc\u atuiesc setul de date ideal pentru acest obiectiv. Desigur, setul de date nu este din categoria celor foarte mari, con\c tin\^ and aproximativ 200.000 de intr\u ari, dar suficiente pentru a modela un limbaj de o anvergur\u a mai mic\u a. Vocabularul setului de date con\c tine 10.000 de cuvinte; cuvintele din afara acestui vocabular sunt marcate cu simbolul unk (unknown). Intr\u arile sunt de forma triplete de fraze \((F_1, F_2, F_3)\), captur\^ and astfel contextul discu\c tiei pe o \^ intindere de trei replici. \^ In cazul \^ in care ar exista o pereche de \(n\) fraze pentru o intrare, HRED este capabil s\u a captureze contextul de-a lungul acestora.

\section{Mediu de lucru/Framework}

\paragraph{}
Pentru dezvoltarea modelului s-a folosit framework-ul Keras cu backend de Tensorflow \^ in limbajul de programare Python. Keras este un framework pentru Deep Learning, preferat de majoritatea comunit\u a\c tii \c stiin\c tifice de Deep Learning, care a fost g\^ andit \c si construit pentru u\c surarea muncii de cercetare \c si prototipizarea rapid\u a a modelelor. Astfel, efortul de concentrare este orientat spre descoperirea arhitecturii potrivite pentru modelarea problemei necesare \c si foarte pu\c tin pe latura de software development. De asemenea, framework-ul este dezvoltat astfel \^ inc\^ at s\u a profite de capacitatea de procesare GPU pe care sistemul hardware o pune la dispozi\c tie, procesul de instruire fiind accelerat de zeci de ori.

\section{Arhitecturi canditat}

\subsection{Arhitectura 1.0}

\paragraph{}
Arhitectura ini\c tial\u a presupunea folosirea unei dimensiuni maxime pentru o fraz\u a de 75 de cuvinte. Deoarece frazele au dimensiuni variabile de lungime, iar o RNAR accept\u a dimensiuni fixe, pentru a completa fraza p\^ an\u a la lungimea maxim\u a aleas\u a, se umpleau spa\c tiile lips\u a cu vectori \(0\), procedeu denumit \c si padding \^ in literatur\u a. Fiecare fraz\u a se sf\^ ar\c se\c ste cu un simbol special numit end token iar apoi urmeaz\u a padding-ul.  Fiecare strat de ie\c sire RNAR din HRED (codor, context \c si decodor) avea o dimensiune de 256 de neuroni. De remarcat este faptul c\u a fiecare unitate structural\u a din HRED, re\c telele recurente folosite pentru codor \c si decodor, foloseau ponderi shared\footnote{Fiecare codor din HRED folosea acela\c si set de ponderi, precum \c si \^ in cazul decodorului}. Stratul de embedding pentru cuvinte pornea cu \^ inv\u a\c tarea de la 0, nefolosindu-se word embeddings provenite de la word2vec. Ie\c sirea din RNAR contextual\u a era concatenat\u a cu reprezentarea fiec\u arui cuv\^ ant din decodor. Ie\c sirea era mai apoi procesat\u a de un strat ascuns cu activare \(\tanh\) pentru expandarea dimensiunii \c si rezultatul era concatenat cu restul secven\c tei de intrare pentru decodor. Algoritmul pentru \^ inv\u a\c tarea ponderilor folosit a fost rmsprop. Pentru predi\c tie s-a folosit\u a varianta greedy descris\u a \^ in Capitolul 3. Aceast\u a versiune performa bine atunci c\^ and erau date pu\c tine, \^ ins\u a odat\u a cu cre\c sterea volumului, modelul nu reu\c sea s\u a scaleze.

\subsection{Arhitectura 1.5}

\paragraph{}
Principala deosebire fa\c t\u a de versiunea precedent\u a a fost folosirea word embeddings pentru reprezentarea cuvintelor \c si inghe\c tarea stratului respectiv pentru a nu modifica ponderile preantrenate. S-a dovedit c\u a aceast\u a alegere a \^ imbun\u at\u a\c tit at\^ at timpul de antrenare, deoarece num\u arul parametrilor a fost redus considerabil, c\^ at \c si performan\c ta modelului, acesta fiind capabil de a captura un num\u ar mai mare de date, dar totu\c si insuficient. Algoritmul pentru optimizarea ponderilor folosit a fost \^ inlocuit cu adam folosind constanta de \^ inv\u a\c tare implic\u a (\(0.001\)), care se presupune c\u a ob\c tine o convergen\c t\u a mai bun\u a fa\c t\u a de rmsprop. De asemenea s-a \^ incercat folosirea simbolului special de \^ inceput de fraz\u a denumit \c si start token, \^ ins\u a s-a observat c\u a ie\c sirea contextual\u a era \^ intotdeauna \^ inclinat\u a c\u atre acest simbol \^ in timpul predic\c tiei, sistemul nemaifiind capabil s\u a modeleze fraza dorit\u a ca r\u aspuns.

\subsection{Arhitectura 2.0}

\paragraph{}
Deoarece modelul e\c sua \^ in a scala atunci c\^ and num\u arul de date cre\c stea, presupunerea a fost c\u a acesta nu era \^ indeajuns de complex pentru a reprezenta distribu\c tia setului de antrenare. De aceea, s-a \^ incercat folosirea unor straturi suplimentare alipite vertical\footnote{Stacked} pentru codor \c si decodor deoarece teoria spune c\u a de la strat la strat, modelul captureaz\u a reprezent\u ari tot mai complexe ale intr\u arii. Procesul de instruire a devenit astfel foarte lent, deoarece complexitatea arhitecturii a crescut, \^ ins\u a rezultatul final era nesatisf\u ac\u ator. 

\subsection{Arhitectura 3.0}

\paragraph{}
Arhitectura 3.0 a adus cu sine un progres important spre atingerea obiectivului final. S-a renun\c tat astfel la straturi stacked deoarece nu rezolvau problema scalabilit\u a\c tii. Noua configura\c tie pentru fiecare ie\c sire RNAR era 300 de neuroni pentru codor, 300 de neuroni pentru context \c si 300 pentru decodor. S-a renun\c tat de asemenea la concatenarea ie\c sirii contextuale cu fiecare reprezentare a cuvintelor din decodor \c si trecerea acesteia prin stratul ascuns cu activare \(\tanh\), deoarece nu mai era nevoie de expandarea dimensiunii. Astfel, ie\c sirea contextual\u a era folosit\u a acum doar ca prim\u a intrare \(T_1\) pentru secven\c ta decodorului, presupunerea fiind c\u a influen\c ta contextual\u a se propag\u a oricum prin RNAR a decodorului. \^ Inc\u a un considerent pentru aceast\u a decizie a fost faptul c\u a ie\c sirea contextual\u a \c si reprezentarea word2vec a cuvintelor provin din distribu\c tii diferite iar \^ impreunarea lor nu reprezenta un factor corect din punct de vedere matematic. Desigur, mecanismele de aten\c tie descrise \^ in Capitolul 5 posed\u a o putere de modelare sporit\u a fa\c t\u a de simpla utilizare a contextului ca prim\u a intrare pentru decodor. \^ Inc\u a o observa\c tie realizat\u a care \^ incetinea procesul de antrenare era folosirea unei fraze cu o lungime maxim\u a de 75 de cuvinte. \^ In urma analizei amanun\c tite a setului de date, s-a observat c\u a majoritatea frazelor aveau o lungime scurt\u a. Astfel, s-a calculat valoarea de \(80\%\) (80 percentile) pentru lungimea maxim\u a a frazelor \c si s-a determinat c\u a aceasta este aproximativ \(25\). Sacrificiul realizat pentru a ignora cuvintele frazelor care dep\u a\c sesc lungimea de 25 de cuvinte a produs o sc\u adere considerabil\u a a timpului de instruire. Odat\u a cu schimbarea c\u atre aceast\u a configura\c tie, s-a adoptat algoritmul beam search de predic\c tie. Deoarece spa\c tiul de c\u autare devenea mai larg, r\u aspunsurile au devenit \c si ele mai diversificate. Valorile pentru num\u arul de copii genera\c ti de fiecare nod \c si beam size au fost alese 2, respectiv 5. \^ In teorie, cu c\^ at aceste numere sunt mai mari, spa\c tiul explorat cre\c ste \c si r\u aspunsul devine mai calitativ. \^ Ins\u a cre\c sterea considerabil\u a a acestor doi parametri aduce cu sine o vitez\u a computa\c tional\u a lent\u a, de aceea trebuie f\u acut un compromis la nivelul acestor valori. \^ In pofida acestor \^ imbun\u at\u a\c tiri, sistemul \^ inc\u a nu scala \^ in mod a\c steptat odat\u a cu cre\c sterea datelor.

\subsection{Arhitectura 3.5}

\paragraph{}
Deoarece setul de date con\c tine subtitr\u ari din filme, este un lucru obi\c snuit ca majoritatea r\u aspunsurilor s\u a fie monotone. Astfel, se creeaz\u a o tendin\c t\u a pentru anumite cuvinte care apar foarte des, cele rare fiind aproape ignorate de c\u atre sistem. De aceea, sistemul e\c sueaz\u a \^ in a furniza r\u aspunsuri diversificate, care s\u a con\c tin\u a \c si cuvinte rare. Acest neajuns se poate rezolva prin folosirea ponderilor pentru cuvinte \^ in momentul c\^ and este calculat\u a valoarea func\c tiei de eroare. Ponderea pentru fiecare cuv\^ ant va fi num\u arul de apari\c tii \(/\) num\u arul total de cuvinte. Prin aceast\u a metod\u a se asigur\u a c\u a sistemul consider\u a influen\c ta egal\u a a tuturor cuvintelor nemaiav\^ and o predispozi\c tie c\u atre cele care apar des. Dimensiunea ie\c sirilor codor, context, decodor au fost alese ca fiind 500, 300, 500 pentru cre\c sterea u\c soar\u a a complexit\u a\c tii modelului. \^ In urma antren\u arii se putea observa c\u a modelul r\u am\^ ane blocat \^ in anumite regiuni de minim local. Acest aspect a fost \^ inl\u aturat prin reducerea progresiv\u a a constantei de \^ inv\u a\c tare de-a lungul instruirii, atunci c\^ and valoarea func\c tiei de eroare nu sc\u adea pe parcursul a cinci epoci consecutive cu mai mult de o valoare \(\epsilon\) aleas\u a. Factorul de reducere a fost ales ca fiind \(75\%\). Aceast\u a combina\c tie de ponderi \^ impreun\u a cu reducerea treptat\u a a constantei de \^ inv\u a\c tare au reprezentat succesul care au produs convergen\c ta modelului curent \c si totodat\u a apari\c tia rezultatelor satisf\u ac\u atoare.







